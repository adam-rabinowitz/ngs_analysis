import pysam
import numpy as np
import pandas as pd
import multiprocessing
import collections

class single_coverage(object):

    def __init__(self, bam):
        self.bam = bam
        bamFile = pysam.AlignmentFile(bam)
        self.length = collections.OrderedDict(
            zip(bamFile.references, bamFile.lengths))
        bamFile.close()

    def check_intervals(self, intervals):
        for chrom, start, end in intervals:
            # Create interval name
            intName = '{}:{}-{}'.format(chrom, start, end)
            # Check chromsome
            if not isinstance(chrom, str):
                raise TypeError('Interval chromosome must be a string')
            try:
                chromLength = self.length[chrom]
            except KeyError:
                raise ValueError('Invalid chromosome: {}'.format(intName))
            # Check start
            if not isinstance(start, int):
                raise TypeError('Interval starts must be integers')
            if not start >= 0:
                raise ValueError('Invalid start: {}'.format(intName))
            # Check end
            if not isinstance(end, int):
                raise TypeError('Interval ends must be integers')
            if not end <= chromLength:
                raise ValueError('Invalid end: {}'.format(intName))
            # Check interval
            if start >= end:
                raise ValueError('Invalid interval: {}'.format(intName))

    def __extract_intervals(
            self, intervals, conn_out, map_quality, remove_dup,
            remove_secondary
        ):
        # Process connections
        conn_out[0].close()
        # Set filter flag to skip unmapped and failed reads
        filter_flag = 516
        # Update flag to skip duplicate and secondary reads, if requested
        if remove_dup:
            filter_flag += 1024
        if remove_secondary:
            filter_flag += 256
        # Open bam file and loop through intervals
        bamFile = pysam.AlignmentFile(self.bam)
        for interval in intervals:
            # Send and extract interval data
            conn_out[1].send(interval)
            chrom, start, end = interval
            # Extract reads for interval
            for read in bamFile.fetch(chrom, start, end):
                # Skip unmapped, failed, duplicate and secondary reads
                if read.flag & filter_flag:
                    continue
                # Skip reads below specified mapping quality
                if read.mapping_quality < map_quality:
                    continue
                # Extract blocks for read
                for block_start, block_end in read.get_blocks():
                    # Skip blocks not overlapping interval
                    if block_end <= start:
                        continue
                    if block_start >= end:
                        continue
                    # Send overlapping blocks
                    conn_out[1].send((
                        max(start, block_start),
                        min(end, block_end)
                    ))
        # Tidy up
        bamFile.close()
        conn_out[1].close()
    
    def __coverage_change(
            self, intervals, conn_out, map_quality, remove_dup,
            remove_secondary
        ):
        # Process output connection and create input connection
        conn_out[0].close()
        conn_in = multiprocessing.Pipe(False)
        # Start process extracting intervals
        process = multiprocessing.Process(
            target = self.__extract_intervals,
            args = (intervals, conn_in, map_quality, remove_dup,
                remove_secondary)
        )
        process.start()
        conn_in[1].close()
        # Process interval data generated by process
        interval = None
        while True:
            try:
                data = conn_in[0].recv()
            except EOFError:
                if interval is not None:
                    conn_out[1].send((interval, cov_change))
                break
            try:
                start, end = data
                cov_change[start] += 1
                cov_change[end] -= 1
            except ValueError:
                if interval is not None:
                    conn_out[1].send((interval, cov_change))
                interval = data
                cov_change = collections.defaultdict(int)
        # Clean up processes and pipes
        conn_in[0].close()
        process.join()
        conn_out[1].close()
    
    def __coverage_array(
            self, intervals, conn_out, map_quality, remove_dup,
            remove_secondary
        ):
        # Proces output connection and create input connection
        conn_out[0].close()
        conn_in = multiprocessing.Pipe(False)
        # Start process extracting coverage change
        process = multiprocessing.Process(
            target = self.__coverage_change,
            args = (intervals, conn_in, map_quality, remove_dup,
                remove_secondary)
        )
        process.start()
        conn_in[1].close()
        # Extract data from pipe
        while True:
            try:
                interval, cov_change = conn_in[0].recv()
            except EOFError:
                break
            # Remove zero change items from coverage
            cov_change = {k:v for k,v in cov_change.items() if v}
            # At interval start and end to coverage
            start, end = interval[1:]
            cov_change[start] = cov_change.get(start, 0)
            cov_change[end] = cov_change.get(end, 0)
            # Convert to numpy array, sort, and remove zeros
            cov_array = np.array(cov_change.items(), dtype=np.int32)
            cov_array = cov_array[cov_array[:,0].argsort()] #Is this needed
            conn_out[1].send((interval, cov_array))
        # Clean up pipes and processes
        conn_in[0].close()
        process.join()
        conn_out[1].close()
    
    def __coverage_array_generator(
            self, intervals, map_quality = 0, remove_dup = False,
            remove_secondary = False, check_intervals = True
        ):
        # Check arguments
        if not isinstance(map_quality, int):
            raise TypeError('map_quality must be integer')
        if map_quality < 0:
            raise ValueError('map_quality must be non-negative')
        if not isinstance(remove_dup, bool):
            raise TypeError('remove_dup must be a bool')
        if not isinstance(remove_secondary, bool):
            raise TypeError('remove_secondary must be a bool')
        if not isinstance(check_intervals, bool):
            raise TypeError('check_intervals must be a bool')
        # Start process building coverage
        conn_in = multiprocessing.Pipe(False)
        process = multiprocessing.Process(
            target = self.__coverage_array,
            args = (intervals, conn_in, map_quality, remove_dup,
                remove_secondary)
        )
        process.start()
        conn_in[1].close()
        # Process coverage generated by process
        while True:
            try:
                cov_array_data = conn_in[0].recv()
            except EOFError:
                break
            yield(cov_array_data)
        # Clean up pipes and processes
        conn_in[0].close()
        process.join()
    
    def bedgraph_file(
            self, bedgraph, intervals, map_quality = 0,
            remove_dup = False
        ):
        # Open output file
        outFile = open(bedgraph, 'w')
        # Create generator and loop through arrays
        for interval, array in self.__coverage_array_generator(
                intervals, map_quality, remove_dup
            ):
            # Calculate coverage
            chrom = interval[0]
            coverage = np.cumsum(array[:,1])
            if coverage[-1] != 0:
                raise ValueError('Error in coverage calculations')
            for i in range(array.shape[0] - 1):
                outLine = '{}\t{}\t{}\t{}\n'.format(
                    chrom,
                    array[i, 0],
                    array[i + 1, 0],
                    coverage[i]
                )
                outFile.write(outLine)
        outFile.close()
    
    def histogram(
            self, intervals, max_cov = 200, map_quality = 0, remove_dup = False
        ):
        # Create ordered dictionary of results
        histogram = collections.OrderedDict()
        for x in range(max_cov + 1):
            histogram[x] = 0
        # Create generator and loop through arrays
        for interval, array in self.__coverage_array_generator(
                intervals, map_quality, remove_dup
            ):
            # Calculate lengths and coverage and add to histogram
            lengths = np.diff(array[:,0])
            coverage = np.cumsum(array[:,1])
            if coverage[-1] != 0:
                raise ValueError('Error in coverage calculations')
            for c, l in zip(coverage[:-1], lengths):
                histogram[min(c, max_cov)] += l
        # Return data
        return(histogram)
    
    def mean_coverage(
            self, intervals, map_quality = 0, remove_dup = False,
            remove_secondary = False, check_intervals = True
        ):
        # Create output series
        names = ['{}:{}-{}'.format(x[0], x[1], x[2]) for x in intervals]
        outSeries = pd.Series(index = names)
        # Create generator and loop through arrays
        for count, (interval, array) in enumerate(self.__coverage_array_generator(
                intervals, map_quality, remove_dup, remove_secondary,
                check_intervals
            )):
            # Calculate lengths and coverage
            lengths = np.diff(array[:,0])
            coverage = np.cumsum(array[:,1])
            if coverage[-1] != 0:
                raise ValueError('Error in coverage calculations')
            # Calculate mean coverage
            meanCov = np.average(coverage[:-1], weights=lengths)
            outSeries[names[count]] = meanCov
        # Clean up processes and pipes
        return(outSeries)

class multiple_coverage(object):
    
    def __init__(self, bamList):
        ''' Function to initialise multiple_coverage object. Object
        functions to perform coverage calculations across BAM files.
        All BAM files must have reference sequences of the same name
        and length.
        
        Args:
            bamList (iter)- Iterable returning path to BAM file.
        
        Raises:
            ValueError - If reference sequences within BAM files
                do not have the same names and lengths.
        
        '''
        # Check bams contain identical reference sequence
        for count, bam in enumerate(bamList):
            bamCov = single_coverage(bam)
            if count:
                if bamCov.length != reference:
                    raise ValueError('Chromosomes in BAM files vary')
            else:
                reference = bamCov.length
        # Store bam list and lengths
        self.length = reference
        self.bamList = bamList
    
    def mean_coverage(
        self, intervals, map_quality=0, remove_dup=False
    ):
        ''' Function to return mean coverage across all intervals
        within matched BAM files.
        
        Args:
            intervals - Iterable of intervals where each element consists
                of a string and two integers specifying chromosome name and
                chromosome start and end, respectively.
            map_quality (int)- Minimum mapping quality for reads.
            remove_dup (bool)- Remove duplicate reads.
        
        Returns:
            outDF - Mean coverage of all intervals in all BAM files.
        
        '''
        # Check intervals
        for count, bam in enumerate(self.bamList):
            # Generate single coverage object
            bamCov = single_coverage(bam)
            # Check intervals for first bam and build output
            if count == 0:
                bamCov.check_intervals(intervals)
                names = ['{}:{}-{}'.format(*x) for x in intervals]
                outDF = pd.DataFrame(index=names, columns=self.bamList)
            # Add coverage to output
            outDF[bam] = bamCov.mean_coverage(
                intervals=intervals, map_quality=map_quality,
                remove_dup=remove_dup, check_intervals=False)
        # Return data
        return(outDF)
    
    def mean_coverage_cor(
        self, intervals, map_quality=0, remove_dup=False, method='pearson',
        min_coverage=0
    ):
        ''' Function to calculate correlation between mean coverage of
        intervals within BAM files.
        
        Args:
            intervals - Iterable of intervals where each element consists
                of a string and two integers specifying chromosome name and
                chromosome start and end, respectively.
            map_quality (int)- Minimum mapping quality for reads.
            remove_dup (bool)- Remove duplicate reads.
            method (str)- Method for correlation calculation.
            min_coverage (int)- Minimum coverage of interval, across all BAM
                files, for inclusion in correlation calculation.
        
        Returns:
            cor - Correlation matrix for bamFiles.
            meanCov - Mean coverage of all intervals in all BAM files.
        
        '''
        # Check arguments
        if not isinstance(return_filtered, bool):
            raise TypeError('return_filtered must be bool')
        methods = ('pearson', 'kendall', 'spearman')
        if method not in methods:
            raise ValueError('method but be one of: {}.'.format(
                ', '.join(methods)))
        # Extract mean coverage and remove low rows
        meanCov = self.mean_coverage(
            intervals=intervals, map_quality=map_quality,
            remove_dup=remove_dup)
        # Perform correlation between remaining points and return
        cor = filtCov.corr(method)
        return((cor, meanCov))
